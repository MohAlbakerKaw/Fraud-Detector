{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit card fraud detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate and process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by downloading and reading in the credit card fraud data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://s3-us-west-2.amazonaws.com/sagemaker-e2e-solutions/fraud-detection/creditcardfraud.zip\n",
    "unzip creditcardfraud.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('creditcard.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at our data (we only show a subset of the columns in the table):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>...</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "5  0.260314 -0.568671  ... -0.208254 -0.559825 -0.026398 -0.371427 -0.232794   \n",
       "6  0.081213  0.464960  ... -0.167716 -0.270710 -0.154104 -0.780055  0.750137   \n",
       "7 -3.807864  0.615375  ...  1.943465 -1.015455  0.057504 -0.649709 -0.415267   \n",
       "8  0.851084 -0.392048  ... -0.073425 -0.268092 -0.204233  1.011592  0.373205   \n",
       "9  0.069539 -0.736727  ... -0.246914 -0.633753 -0.120794 -0.385050 -0.069733   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "5  0.105915  0.253844  0.081080    3.67      0  \n",
       "6 -0.257237  0.034507  0.005168    4.99      0  \n",
       "7 -0.051634 -1.206921 -1.085339   40.80      0  \n",
       "8 -0.384157  0.011747  0.142404   93.20      0  \n",
       "9  0.094199  0.246219  0.083076    3.68      0  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.columns)\n",
    "data[['Time', 'V1', 'V2', 'V27', 'V28', 'Amount', 'Class']].describe()\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class column corresponds to whether or not a transaction is fradulent. We see that the majority of data is non-fraudulant with only $492$ ($.173\\%$) of the data corresponding to fraudulant examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frauds:  492\n",
      "Number of non-frauds:  284315\n",
      "Percentage of fradulent data: 0.1727485630620034\n"
     ]
    }
   ],
   "source": [
    "nonfrauds, frauds = data.groupby('Class').size()\n",
    "print('Number of frauds: ', frauds)\n",
    "print('Number of non-frauds: ', nonfrauds)\n",
    "print('Percentage of fradulent data:', 100.*frauds/(frauds + nonfrauds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 28 columns, $V_i$ for $i=1..28$ of anonymized features along with columns for time, amount, and class. We already know that the columns $V_i$ have been normalized to have $0$ mean and unit standard deviation as the result of a PCA. You can read more about PCA here:. \n",
    "\n",
    "Tip: For our dataset this amount of preprocessing will give us reasonable accuracy, but it's important to note that there are more preprocessing steps one can use to improve accuracy . For unbalanced data sets like ours where the positive (fraudulent) examples occur much less frequently than the negative (legitimate) examples, we may try “over-sampling” the minority dataset by generating synthetic data (read about SMOTE in Data Mining for Imbalanced Datasets: An Overview (https://link.springer.com/chapter/10.1007%2F0-387-25465-X_40) or undersampling the majority class by using ensemble methods (see http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.6858&rep=rep1&type=pdfor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = data.columns[:-1]\n",
    "label_column = data.columns[-1]\n",
    "\n",
    "features = data[feature_columns].values.astype('float32')\n",
    "labels = (data[label_column].values).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some analysis and discuss different ways we can preprocess our data. Let's discuss the way in which this data was preprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data and Upload to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Amazon common libraries provide utilities to convert NumPy n-dimensional arrays into a the Record-IO format which SageMaker uses for a concise representation of features and labels. The Record-IO format is implemented via protocol buffer so the serialization is very efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0      0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1      0   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2      0   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3      0   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4      0   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "\n",
       "         V7        V8  ...       V20       V21       V22       V23       V24  \\\n",
       "0  0.239599  0.098698  ...  0.251412 -0.018307  0.277838 -0.110474  0.066928   \n",
       "1 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.791461  0.247676  ...  0.524980  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.237609  0.377436  ... -0.208038 -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data = data\n",
    "model_data.head()\n",
    "model_data = pd.concat([model_data['Class'], model_data.drop(['Class'], axis=1)], axis=1)\n",
    "model_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we upload the data to S3 using boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training data location: s3://sagemaker-us-east-1-282128611277/sagemaker/DEMO-xgboost-fraud/train/train.csv\n",
      "Uploaded training data location: s3://sagemaker-us-east-1-282128611277/sagemaker/DEMO-xgboost-fraud/validation/validation.csv\n",
      "Training artifacts will be uploaded to: s3://sagemaker-us-east-1-282128611277/sagemaker/DEMO-xgboost-fraud/output\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "sagemaker_iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "prefix = 'sagemaker/DEMO-xgboost-fraud'\n",
    "\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), \n",
    "                                                  [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)\n",
    "\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')) \\\n",
    "                                .upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')) \\\n",
    "                                .upload_file('validation.csv')\n",
    "s3_train_data = 's3://{}/{}/train/train.csv'.format(bucket, prefix)\n",
    "s3_validation_data = 's3://{}/{}/validation/validation.csv'.format(bucket, prefix)\n",
    "print('Uploaded training data location: {}'.format(s3_train_data))\n",
    "print('Uploaded training data location: {}'.format(s3_validation_data))\n",
    "\n",
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('Training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train\n",
    "\n",
    "Moving onto training, first we'll need to specify the locations of the XGBoost algorithm containers.\n",
    "To specify the Linear Learner algorithm, we use a utility function to obtain it's URI. A complete list of build-in algorithms is found here: https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is a more up to date SageMaker XGBoost image.To use the newer image, please set 'repo_version'='0.90-1. For example:\n",
      "\tget_image_uri(region, 'xgboost', '0.90-1').\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create s3_inputs that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify a few parameters like what type of training instances we'd like to use and how many, as well as our XGBoost hyperparameters.  A few key hyperparameters are:\n",
    "- `max_depth` controls how deep each tree within the algorithm can be built.  Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting.  There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "- `subsample` controls sampling of the training data.  This technique can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "- `num_round` controls the number of boosting rounds.  This is essentially the subsequent models that are trained using the residuals of previous iterations.  Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `eta` controls how aggressive each round of boosting is.  Larger values lead to more conservative boosting.\n",
    "- `gamma` controls how aggressively trees are grown.  Larger values lead to more conservative models.\n",
    "\n",
    "More detail on XGBoost's hyperparmeters can be found on their GitHub [page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker abstracts training with Estimators. We can pass container, and all parameters to the estimator, as well as the hyperparameters for the linear learner and fit the estimator to the data in S3.\n",
    "Note: For IP protection reasons, SageMaker built-in algorithms, such as XGBoost, can't be run locally, i.e. on the same instance where this Jupyter Notebook code is running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role=sagemaker_iam_role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    sagemaker_session=session)\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host XGBoost Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we deploy the estimator to and endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Using already existing model: xgboost-2019-12-03-21-58-34-726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "xgb.name = 'deployed-xgboost-fraud-prediction'\n",
    "xgb_predictor = xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge',\n",
    "                          endpoint_name='deployed-xgboost-fraud-prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model very easily, \n",
    "simply by making an http POST request.  But first, we'll need to setup serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer \n",
    "\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batchs to CSV string payloads\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(test_data.as_matrix()[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to compare the performance of a machine learning model, but let's start by simply by comparing actual to predicted values. In this case, we're simply predicting whether the customer churned (1) or not (0), which produces a simple confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frauds:  54\n",
      "Number of non-frauds:  28427\n",
      "Percentage of fradulent data: 0.1896000842667041\n"
     ]
    }
   ],
   "source": [
    "test_nonfrauds, test_frauds = test_data.groupby('Class').size()\n",
    "print('Number of frauds: ', test_frauds)\n",
    "print('Number of non-frauds: ', test_nonfrauds)\n",
    "print('Percentage of fradulent data:', 100.*test_frauds/(test_frauds + test_nonfrauds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28426</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions    0.0  1.0\n",
       "actual                 \n",
       "0            28426    1\n",
       "1               14   40"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(predictions), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.98\n",
      "recall:  0.74\n"
     ]
    }
   ],
   "source": [
    "#precision: tp / (tp + fp)\n",
    "#recall: tp / (tp + fn)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "results = precision_recall_fscore_support(test_data.iloc[:, 0],\n",
    "                                         np.round(predictions))\n",
    "print('precision: ', round(results[0][1], 2))\n",
    "print('recall: ', round(results[1][1], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, due to randomized elements of the algorithm, you results may differ slightly.\n",
    "\n",
    "Of the 54 fraudsters, we've correctly predicted 40 of them (true positives). And, we incorrectly predicted 1 case of fraud (false positive). There are also 14 cases of fraud that the model classified as benign transaction (false negatives) - which can get really expensive.\n",
    "\n",
    "An important point here is that because of the np.round() function above we are using a simple threshold (or cutoff) of 0.5. Our predictions from xgboost come out as continuous values between 0 and 1 and we force them into the binary classes that we began with. So, we should consider adjusting this cutoff. That will almost certainly increase the number of false positives, but it can also be expected to increase the number of true positives and reduce the number of false negatives.\n",
    "\n",
    "To get a rough intuition here, let's look at the continuous values of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the cutoff threshold, we can trade false positives for false negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28411</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0      0   1\n",
       "Class           \n",
       "0      28411  16\n",
       "1          7  47"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.where(predictions > 0.04, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.75\n",
      "recall:  0.87\n"
     ]
    }
   ],
   "source": [
    "results = precision_recall_fscore_support(test_data.iloc[:, 0],\n",
    "                                         np.where(predictions > 0.04, 1, 0))\n",
    "print('precision: ', round(results[0][1], 2))\n",
    "print('recall: ', round(results[1][1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative cost of errors\n",
    "\n",
    "Any practical binary classification problem is likely to produce a similarly sensitive cutoff. \n",
    "If we put an ML model into production, there are costs associated with the model erroneously assigning false positives and false negatives. Because the choice of the cutoff affects all four of these statistics, we need to consider the relative costs to the business for each of these four outcomes for each prediction.\n",
    "\n",
    "#### Assigning costs\n",
    "\n",
    "What are the costs for our problem fraud detection? The costs, of course, depend on the specific actions that the business takes. Let's make some assumptions here.\n",
    "\n",
    "First, assign the cost of \\$0.00 to both the true negatives (correctly recognized benign transactions) and true positives (correctly recognized fraudulent transactions). Our model essentially correctly identified both situations. One can assign a benefit (i.e. negative cost) to correctly detected fraud, but we are not going to do this here.\n",
    "\n",
    "False negatives are the most problematic, because they represent a fraudulent transactions that slipped through our model. Based on some Internet research (see sources below), we assign a cost of \\$450.00 for each one. This is the cost of false negatives.\n",
    "\n",
    "Finally, False positives are the genuine transactions that our model would block as fraud. This would result in an annoyed customer that might possibly close the credit card account and move to another bank. We assume that it costs a \\$500.00 sign-on bonus to obtain a cr. card customer and that \\5 percent of annoyed customers would defect. \n",
    "\n",
    "Source:\n",
    "\n",
    "https://www.creditcards.com/credit-card-news/credit-card-security-id-theft-fraud-statistics-1276.php\n",
    "https://wallethub.com/edu/cc/credit-debit-card-fraud-statistics/25725/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal cutoff\n",
    "\n",
    "It’s clear that false negatives are substantially more costly than false positives. We should be minimizing a cost function that looks like this:\n",
    "\n",
    "```txt\n",
    "$450 * FN(C) + $0 * TN(C) + 0.05*$500 * FP(C) + $0 * TP(C)\n",
    "```\n",
    "\n",
    "FN(C) means that the false negative percentage is a function of the cutoff, C, and similar for TN, FP, and TP.  We need to find the cutoff, C, where the result of the expression is smallest.\n",
    "\n",
    "A straightforward way to do this, is to simply run a simulation over a large number of possible cutoffs.  We test 100 possible values in the for loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is minimized near a cutoff of: 0.04\n"
     ]
    }
   ],
   "source": [
    "TN_cost = 0\n",
    "TP_cost = 0\n",
    "FP_cost = 0.05*500 #$cost of losing an annoyed customer (assuming 5% defection and $500 sign-on bonus)\n",
    "FN_cost = 450 # $cost of of letting a fradulent transaction slip through\n",
    "\n",
    "cutoffs = np.arange(0.01, 1, 0.01)\n",
    "costs = []\n",
    "for c in cutoffs:\n",
    "    costs.append(np.sum(np.sum(np.array([[TN_cost, FP_cost], [FN_cost, TP_cost]]) * \n",
    "                               pd.crosstab(index=test_data.iloc[:, 0], \n",
    "                                           columns=np.where(predictions > c, 1, 0)))))\n",
    "\n",
    "costs = np.array(costs)\n",
    "plt.plot(cutoffs, costs)\n",
    "plt.show()\n",
    "print('Cost is minimized near a cutoff of:', cutoffs[np.argmin(costs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "We will leave the prediction endpoint running at the end of this notebook so we can handle incoming event streams. However, don't forget to delete the prediction endpoint when you're done. You can do that at the Amazon SageMaker console in the Endpoints page. Or you can run `xgb_predictor.delete_endpoint()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Acknowledgements\n",
    "\n",
    "The dataset used to demonstrated the fraud detection solution has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the [DefeatFraud](https://mlg.ulb.ac.be/wordpress/portfolio_page/defeatfraud-assessment-and-validation-of-deep-feature-engineering-and-learning-solutions-for-fraud-detection/) project\n",
    "We cite the following works:\n",
    "* Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "* Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
    "* Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
    "* Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n",
    "* Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Aël; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n",
    "* Carcillo, Fabrizio; Le Borgne, Yann-Aël; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
